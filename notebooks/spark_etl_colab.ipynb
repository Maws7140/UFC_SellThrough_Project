{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# UFC Spark ETL Pipeline - Google Colab\n",
        "\n",
        "This notebook runs the Spark ETL pipeline for UFC data processing on Google Colab.\n",
        "\n",
        "## Setup Steps:\n",
        "1. Install PySpark\n",
        "2. Upload your data files or mount Google Drive\n",
        "3. Run the ETL pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Install PySpark\n",
        "%pip install pyspark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Mount Google Drive\n",
        "# Upload your 'data' folder to Google Drive first, then run this cell\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set your data directory path - adjust this to match your Google Drive folder structure\n",
        "DATA_DIR = '/content/drive/MyDrive/UFC_SellThrough_Project/data'\n",
        "OUTPUT_DIR = '/content/drive/MyDrive/UFC_SellThrough_Project/data'\n",
        "\n",
        "print(f\"Data directory: {DATA_DIR}\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Verify data files exist\n",
        "import os\n",
        "\n",
        "print(\"Raw data files:\")\n",
        "raw_path = os.path.join(DATA_DIR, 'raw')\n",
        "if os.path.exists(raw_path):\n",
        "    for f in os.listdir(raw_path):\n",
        "        if f.endswith('.csv'):\n",
        "            print(f\"  - {f}\")\n",
        "else:\n",
        "    print(f\"  Directory not found: {raw_path}\")\n",
        "\n",
        "print(\"\\nExternal data files:\")\n",
        "ext_path = os.path.join(DATA_DIR, 'external')\n",
        "if os.path.exists(ext_path):\n",
        "    for f in os.listdir(ext_path):\n",
        "        if f.endswith('.csv'):\n",
        "            print(f\"  - {f}\")\n",
        "else:\n",
        "    print(f\"  Directory not found: {ext_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Import libraries and setup Spark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import DateType, IntegerType, FloatType, BooleanType\n",
        "import os\n",
        "\n",
        "def make_spark(app_name=\"UFC-ETL\", local=True):\n",
        "    builder = SparkSession.builder.appName(app_name)\n",
        "    \n",
        "    if local:\n",
        "        builder = builder.master(\"local[*]\")\n",
        "    \n",
        "    # Colab-optimized settings (reduced memory for Colab's limited RAM)\n",
        "    builder = (builder\n",
        "        .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
        "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
        "        .config(\"spark.sql.parquet.compression.codec\", \"snappy\")\n",
        "        .config(\"spark.driver.memory\", \"2g\")\n",
        "        .config(\"spark.executor.memory\", \"2g\")\n",
        "    )\n",
        "    \n",
        "    return builder.getOrCreate()\n",
        "\n",
        "# Create Spark session\n",
        "spark = make_spark()\n",
        "print(f\"Spark version: {spark.version}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5: Define data loading functions\n",
        "\n",
        "def load_data(spark, data_dir):\n",
        "    print(\"Loading CSV files...\")\n",
        "    raw_path = os.path.join(data_dir, \"raw\")\n",
        "    \n",
        "    file_map = {\n",
        "        \"events\": \"events.csv\",\n",
        "        \"fight_results\": \"fight_results.csv\",\n",
        "        \"fight_stats\": \"fight_stats.csv\",\n",
        "        \"fight_details\": \"fight_details.csv\",\n",
        "        \"fighter_details\": \"fighter_details.csv\",\n",
        "        \"fighter_tott\": \"fighter_tott.csv\",\n",
        "    }\n",
        "    \n",
        "    data = {}\n",
        "    for name, fname in file_map.items():\n",
        "        full_path = os.path.join(raw_path, fname)\n",
        "        if os.path.exists(full_path):\n",
        "            df = spark.read.csv(full_path, header=True, inferSchema=True)\n",
        "            for col in df.columns:\n",
        "                df = df.withColumnRenamed(col, col.lower())\n",
        "            print(f\"  Loaded {name}: {df.count()} rows\")\n",
        "            data[name] = df\n",
        "        else:\n",
        "            print(f\"  Warning: {full_path} not found\")\n",
        "    return data\n",
        "\n",
        "def load_external_data(spark, data_dir):\n",
        "    print(\"Loading external data sources...\")\n",
        "    external_path = os.path.join(data_dir, \"external\")\n",
        "    external_data = {}\n",
        "    \n",
        "    external_files = {\n",
        "        \"betting_odds\": \"betting_odds.csv\",\n",
        "        \"google_trends\": \"google_trends.csv\",\n",
        "        \"fighter_buzz\": \"fighter_buzz.csv\",\n",
        "        \"event_sentiment\": \"event_sentiment.csv\",\n",
        "        \"reddit_comments\": \"reddit_comments.csv\",\n",
        "    }\n",
        "    \n",
        "    for name, fname in external_files.items():\n",
        "        full_path = os.path.join(external_path, fname)\n",
        "        if os.path.exists(full_path):\n",
        "            df = spark.read.csv(full_path, header=True, inferSchema=True)\n",
        "            for col in df.columns:\n",
        "                df = df.withColumnRenamed(col, col.lower())\n",
        "            print(f\"  Loaded {name}: {df.count()} rows\")\n",
        "            external_data[name] = df\n",
        "        else:\n",
        "            print(f\"  Note: {fname} not found\")\n",
        "    return external_data\n",
        "\n",
        "def load_graph_features(spark, data_dir):\n",
        "    print(\"Loading graph features...\")\n",
        "    graph_path = os.path.join(data_dir, \"features\", \"graph_features\")\n",
        "    \n",
        "    if os.path.exists(graph_path):\n",
        "        df = spark.read.parquet(graph_path)\n",
        "        print(f\"  Loaded graph features: {df.count()} fighters\")\n",
        "        return df\n",
        "    else:\n",
        "        print(\"  Note: Graph features not found\")\n",
        "        return None\n",
        "\n",
        "print(\"Data loading functions defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 6: Define data cleaning functions\n",
        "\n",
        "def clean_events(df):\n",
        "    print(\"Cleaning events...\")\n",
        "    df = df.withColumn(\"event_date\", F.to_date(F.col(\"date\"), \"MMMM d, yyyy\"))\n",
        "    df = df.withColumn(\n",
        "        \"event_type\",\n",
        "        F.when(F.col(\"event\").rlike(\"^UFC [0-9]+\"), \"PPV\")\n",
        "         .when(F.col(\"event\").contains(\"Fight Night\"), \"Fight Night\")\n",
        "         .when(F.col(\"event\").contains(\"TUF\"), \"TUF\")\n",
        "         .when(F.col(\"event\").contains(\"Contender\"), \"DWCS\")\n",
        "         .otherwise(\"Other\")\n",
        "    )\n",
        "    df = df.withColumn(\"location_parts\", F.split(F.col(\"location\"), \", \"))\n",
        "    df = df.withColumn(\"city\", F.element_at(F.col(\"location_parts\"), 1))\n",
        "    df = df.withColumn(\"state_country\", F.element_at(F.col(\"location_parts\"), 2))\n",
        "    df = df.withColumn(\n",
        "        \"country\",\n",
        "        F.when(F.col(\"state_country\").isin([\"USA\", \"United States\"]), \"USA\")\n",
        "         .when(F.col(\"state_country\").isin([\"UK\", \"England\", \"Scotland\"]), \"UK\")\n",
        "         .when(F.col(\"state_country\").isin([\"Canada\"]), \"Canada\")\n",
        "         .otherwise(F.col(\"state_country\"))\n",
        "    )\n",
        "    df = df.withColumn(\"event_id\", F.regexp_extract(F.col(\"url\"), r\"event/([^/]+)\", 1))\n",
        "    df = df.withColumn(\"year\", F.year(F.col(\"event_date\")))\n",
        "    df = df.select(\n",
        "        F.col(\"event_id\"), F.col(\"event\").alias(\"event_name\"),\n",
        "        F.col(\"event_date\"), F.col(\"event_type\"), F.col(\"city\"),\n",
        "        F.col(\"country\"), F.col(\"location\"), F.col(\"year\")\n",
        "    )\n",
        "    return df.dropDuplicates([\"event_id\"])\n",
        "\n",
        "def clean_fighters(df):\n",
        "    print(\"Cleaning fighters...\")\n",
        "    df = df.withColumn(\"fighter_id\", F.regexp_extract(F.col(\"url\"), r\"fighter/([^/]+)\", 1))\n",
        "    df = df.withColumn(\n",
        "        \"height_inches\",\n",
        "        F.regexp_extract(F.col(\"height\"), r\"(\\d+)'\", 1).cast(IntegerType()) * 12 +\n",
        "        F.regexp_extract(F.col(\"height\"), r'(\\d+)\"', 1).cast(IntegerType())\n",
        "    )\n",
        "    df = df.withColumn(\"reach_inches\", F.regexp_extract(F.col(\"reach\"), r\"(\\d+)\", 1).cast(IntegerType()))\n",
        "    df = df.withColumn(\"weight_lbs\", F.regexp_extract(F.col(\"weight\"), r\"(\\d+)\", 1).cast(IntegerType()))\n",
        "    df = df.withColumn(\"dob\", F.to_date(F.col(\"dob\"), \"MMM dd, yyyy\"))\n",
        "    \n",
        "    if \"record\" in [c.lower() for c in df.columns]:\n",
        "        df = df.withColumn(\"wins\", F.regexp_extract(F.col(\"record\"), r\"^(\\d+)\", 1).cast(IntegerType()))\n",
        "        df = df.withColumn(\"losses\", F.regexp_extract(F.col(\"record\"), r\"^(\\d+)-(\\d+)\", 2).cast(IntegerType()))\n",
        "        df = df.withColumn(\"draws\", F.regexp_extract(F.col(\"record\"), r\"^(\\d+)-(\\d+)-(\\d+)\", 3).cast(IntegerType()))\n",
        "    else:\n",
        "        df = df.withColumn(\"wins\", F.lit(None).cast(IntegerType()))\n",
        "        df = df.withColumn(\"losses\", F.lit(None).cast(IntegerType()))\n",
        "        df = df.withColumn(\"draws\", F.lit(None).cast(IntegerType()))\n",
        "    \n",
        "    if \"fighter\" in df.columns:\n",
        "        df = df.withColumn(\"fighter_name\", F.col(\"fighter\"))\n",
        "    elif \"first\" in df.columns and \"last\" in df.columns:\n",
        "        df = df.withColumn(\"fighter_name\", F.concat_ws(\" \", F.col(\"first\"), F.col(\"last\")))\n",
        "    else:\n",
        "        df = df.withColumn(\"fighter_name\", F.lit(\"Unknown\"))\n",
        "    \n",
        "    df = df.withColumn(\"nickname\", F.col(\"nickname\") if \"nickname\" in df.columns else F.lit(None).cast(\"string\"))\n",
        "    df = df.withColumn(\"stance\", F.col(\"stance\") if \"stance\" in df.columns else F.lit(None).cast(\"string\"))\n",
        "    \n",
        "    df = df.select(\n",
        "        \"fighter_id\", \"fighter_name\", \"nickname\", \"height_inches\",\n",
        "        \"weight_lbs\", \"reach_inches\", \"stance\", \"dob\", \"wins\", \"losses\", \"draws\"\n",
        "    )\n",
        "    return df.dropDuplicates([\"fighter_id\"])\n",
        "\n",
        "print(\"Event and fighter cleaning functions defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 7: Define fight cleaning functions\n",
        "\n",
        "def clean_fights(df):\n",
        "    print(\"Cleaning fights...\")\n",
        "    df = df.withColumn(\"fight_id\", F.regexp_extract(F.col(\"url\"), r\"fight/([^/]+)\", 1))\n",
        "    \n",
        "    if \"event_url\" in df.columns:\n",
        "        df = df.withColumn(\"event_id\", F.regexp_extract(F.col(\"event_url\"), r\"event/([^/]+)\", 1))\n",
        "    else:\n",
        "        df = df.withColumn(\"event_id\", F.lit(None).cast(\"string\"))\n",
        "    \n",
        "    wc_col = \"weightclass\" if \"weightclass\" in df.columns else \"weight_class\"\n",
        "    if wc_col in df.columns:\n",
        "        df = df.withColumn(\"is_title_fight\", F.col(wc_col).contains(\"Title\").cast(BooleanType()))\n",
        "        df = df.withColumn(\"weight_class_clean\", F.regexp_replace(F.col(wc_col), \" Title Bout\", \"\"))\n",
        "    else:\n",
        "        df = df.withColumn(\"is_title_fight\", F.lit(False))\n",
        "        df = df.withColumn(\"weight_class_clean\", F.lit(\"Unknown\"))\n",
        "    \n",
        "    if \"method\" in df.columns:\n",
        "        df = df.withColumn(\n",
        "            \"method_category\",\n",
        "            F.when(F.col(\"method\").contains(\"KO\"), \"KO/TKO\")\n",
        "             .when(F.col(\"method\").contains(\"SUB\"), \"Submission\")\n",
        "             .when(F.col(\"method\").contains(\"DEC\"), \"Decision\")\n",
        "             .otherwise(\"Other\")\n",
        "        )\n",
        "    else:\n",
        "        df = df.withColumn(\"method\", F.lit(None).cast(\"string\"))\n",
        "        df = df.withColumn(\"method_category\", F.lit(\"Unknown\"))\n",
        "    \n",
        "    if \"round\" in df.columns:\n",
        "        df = df.withColumn(\"round_num\", F.regexp_extract(F.col(\"round\"), r\"(\\d+)\", 1).cast(IntegerType()))\n",
        "    else:\n",
        "        df = df.withColumn(\"round_num\", F.lit(None).cast(IntegerType()))\n",
        "    \n",
        "    if \"bout\" in df.columns:\n",
        "        df = df.withColumn(\"fighter1_name\", F.trim(F.regexp_extract(F.col(\"bout\"), r\"^(.+?)\\s+vs\\.?\\s+\", 1)))\n",
        "        df = df.withColumn(\"fighter2_name\", F.trim(F.regexp_extract(F.col(\"bout\"), r\"\\s+vs\\.?\\s+(.+)$\", 1)))\n",
        "    elif \"fighter1\" in df.columns and \"fighter2\" in df.columns:\n",
        "        df = df.withColumn(\"fighter1_name\", F.col(\"fighter1\"))\n",
        "        df = df.withColumn(\"fighter2_name\", F.col(\"fighter2\"))\n",
        "    else:\n",
        "        df = df.withColumn(\"fighter1_name\", F.lit(\"Unknown\"))\n",
        "        df = df.withColumn(\"fighter2_name\", F.lit(\"Unknown\"))\n",
        "    \n",
        "    if \"winner\" in df.columns:\n",
        "        df = df.withColumn(\"winner_name\", F.col(\"winner\"))\n",
        "    elif \"outcome\" in df.columns:\n",
        "        df = df.withColumn(\n",
        "            \"winner_name\",\n",
        "            F.when(F.col(\"outcome\").startswith(\"W\"), F.col(\"fighter1_name\"))\n",
        "             .when(F.col(\"outcome\").startswith(\"L\"), F.col(\"fighter2_name\"))\n",
        "             .otherwise(F.lit(None).cast(\"string\"))\n",
        "        )\n",
        "    else:\n",
        "        df = df.withColumn(\"winner_name\", F.lit(None).cast(\"string\"))\n",
        "    \n",
        "    df = df.withColumn(\"finish_time\", F.col(\"time\") if \"time\" in df.columns else F.lit(None).cast(\"string\"))\n",
        "    \n",
        "    df = df.select(\n",
        "        \"fight_id\", \"event_id\", \"fighter1_name\", \"fighter2_name\", \"winner_name\",\n",
        "        F.col(\"weight_class_clean\").alias(\"weight_class\"), \"is_title_fight\",\n",
        "        \"method_category\", \"method\", F.col(\"round_num\").alias(\"round\"), \"finish_time\"\n",
        "    )\n",
        "    return df.dropDuplicates([\"fight_id\"])\n",
        "\n",
        "def clean_fight_stats(df):\n",
        "    print(\"Cleaning fight stats...\")\n",
        "    cols = [c.lower() for c in df.columns]\n",
        "    \n",
        "    df = df.withColumn(\"fighter_name\", F.col(\"fighter\") if \"fighter\" in cols else F.lit(\"Unknown\"))\n",
        "    \n",
        "    if \"event\" in cols and \"bout\" in cols:\n",
        "        df = df.withColumn(\"fight_key\", F.concat_ws(\"_\", F.col(\"event\"), F.col(\"bout\")))\n",
        "    else:\n",
        "        df = df.withColumn(\"fight_key\", F.lit(None).cast(\"string\"))\n",
        "    \n",
        "    sig_str_col = None\n",
        "    for c in df.columns:\n",
        "        if \"sig\" in c.lower() and \"str\" in c.lower():\n",
        "            sig_str_col = c\n",
        "            break\n",
        "    \n",
        "    if sig_str_col:\n",
        "        df = df.withColumn(\"sig_strikes_landed\", F.regexp_extract(F.col(sig_str_col), r\"(\\d+) of\", 1).cast(IntegerType()))\n",
        "        df = df.withColumn(\"sig_strikes_attempted\", F.regexp_extract(F.col(sig_str_col), r\"of (\\d+)\", 1).cast(IntegerType()))\n",
        "    else:\n",
        "        df = df.withColumn(\"sig_strikes_landed\", F.lit(None).cast(IntegerType()))\n",
        "        df = df.withColumn(\"sig_strikes_attempted\", F.lit(None).cast(IntegerType()))\n",
        "    \n",
        "    if \"td\" in cols:\n",
        "        df = df.withColumn(\"takedowns_landed\", F.regexp_extract(F.col(\"td\"), r\"(\\d+) of\", 1).cast(IntegerType()))\n",
        "        df = df.withColumn(\"takedowns_attempted\", F.regexp_extract(F.col(\"td\"), r\"of (\\d+)\", 1).cast(IntegerType()))\n",
        "    else:\n",
        "        df = df.withColumn(\"takedowns_landed\", F.lit(None).cast(IntegerType()))\n",
        "        df = df.withColumn(\"takedowns_attempted\", F.lit(None).cast(IntegerType()))\n",
        "    \n",
        "    df = df.withColumn(\"knockdowns\", F.col(\"kd\").cast(IntegerType()) if \"kd\" in cols else F.lit(None).cast(IntegerType()))\n",
        "    \n",
        "    return df\n",
        "\n",
        "print(\"Fight cleaning functions defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 8: Define feature processing functions\n",
        "\n",
        "def build_event_features(events_df, fights_df):\n",
        "    print(\"Building event features...\")\n",
        "    fight_agg = fights_df.groupBy(\"event_id\").agg(\n",
        "        F.count(\"*\").alias(\"num_fights\"),\n",
        "        F.sum(F.col(\"is_title_fight\").cast(IntegerType())).alias(\"num_title_fights\"),\n",
        "        F.first(F.when(F.col(\"is_title_fight\"), F.col(\"weight_class\"))).alias(\"title_weight_class\")\n",
        "    )\n",
        "    df = events_df.join(fight_agg, on=\"event_id\", how=\"left\")\n",
        "    df = df.withColumn(\"day_of_week\", F.dayofweek(\"event_date\"))\n",
        "    df = df.withColumn(\"month\", F.month(\"event_date\"))\n",
        "    df = df.withColumn(\"is_saturday\", (F.col(\"day_of_week\") == 7).cast(IntegerType()))\n",
        "    df = df.withColumn(\"is_las_vegas\", F.col(\"city\").contains(\"Las Vegas\").cast(IntegerType()))\n",
        "    df = df.withColumn(\"is_usa\", (F.col(\"country\") == \"USA\").cast(IntegerType()))\n",
        "    return df\n",
        "\n",
        "def process_betting_odds(betting_df, fights_df):\n",
        "    if betting_df is None:\n",
        "        return None\n",
        "    print(\"Processing betting odds...\")\n",
        "    betting_df = betting_df.withColumn(\"f1_name_norm\", F.lower(F.regexp_replace(F.col(\"fighter1_name\"), r\"[^a-z0-9]\", \"\")))\n",
        "    betting_df = betting_df.withColumn(\"f2_name_norm\", F.lower(F.regexp_replace(F.col(\"fighter2_name\"), r\"[^a-z0-9]\", \"\")))\n",
        "    betting_features = betting_df.select(\n",
        "        \"f1_name_norm\", \"f2_name_norm\",\n",
        "        F.col(\"fighter1_odds\").alias(\"f1_closing_odds\"),\n",
        "        F.col(\"fighter2_odds\").alias(\"f2_closing_odds\"),\n",
        "        F.col(\"fighter1_implied_prob\").alias(\"f1_implied_prob\"),\n",
        "        F.col(\"fighter2_implied_prob\").alias(\"f2_implied_prob\"),\n",
        "        F.col(\"odds_spread\").alias(\"betting_spread\"),\n",
        "        F.when(F.col(\"is_competitive_matchup\") == 1, True).otherwise(False).alias(\"is_competitive\"),\n",
        "        F.when(F.col(\"has_heavy_favorite\") == 1, True).otherwise(False).alias(\"has_heavy_favorite\")\n",
        "    )\n",
        "    print(f\"  Processed {betting_features.count()} betting records\")\n",
        "    return betting_features\n",
        "\n",
        "def process_sentiment(sentiment_df, events_df):\n",
        "    if sentiment_df is None:\n",
        "        return None\n",
        "    print(\"Processing event sentiment...\")\n",
        "    sentiment_df = sentiment_df.withColumn(\"event_name_norm\", F.lower(F.regexp_replace(F.col(\"event_name\"), r\"[^a-z0-9]\", \"\")))\n",
        "    sentiment_features = sentiment_df.select(\n",
        "        \"event_name_norm\",\n",
        "        F.col(\"avg_sentiment\").alias(\"reddit_sentiment\"),\n",
        "        F.col(\"sentiment_std\").alias(\"sentiment_variance\"),\n",
        "        F.col(\"total_engagement\").alias(\"reddit_engagement\"),\n",
        "        F.col(\"comment_count\").alias(\"reddit_comments\"),\n",
        "        F.col(\"hype_score\").alias(\"reddit_hype\")\n",
        "    )\n",
        "    print(f\"  Processed {sentiment_features.count()} sentiment records\")\n",
        "    return sentiment_features\n",
        "\n",
        "def process_trends(trends_df, buzz_df):\n",
        "    if buzz_df is None and trends_df is None:\n",
        "        return None\n",
        "    print(\"Processing Google Trends...\")\n",
        "    if buzz_df is not None:\n",
        "        buzz_df = buzz_df.withColumn(\"f1_name_norm\", F.lower(F.regexp_replace(F.col(\"fighter1_name\"), r\"[^a-z0-9]\", \"\")))\n",
        "        buzz_df = buzz_df.withColumn(\"f2_name_norm\", F.lower(F.regexp_replace(F.col(\"fighter2_name\"), r\"[^a-z0-9]\", \"\")))\n",
        "        trends_features = buzz_df.select(\n",
        "            \"f1_name_norm\", \"f2_name_norm\",\n",
        "            F.col(\"combined_buzz_7d\").alias(\"pre_event_buzz_7d\"),\n",
        "            F.col(\"combined_buzz_30d\").alias(\"pre_event_buzz_30d\"),\n",
        "            F.col(\"max_peak_search\").alias(\"peak_search_interest\"),\n",
        "            F.col(\"buzz_differential\").alias(\"buzz_differential\")\n",
        "        )\n",
        "        print(f\"  Processed {trends_features.count()} buzz records\")\n",
        "        return trends_features\n",
        "    if trends_df is not None:\n",
        "        trends_agg = trends_df.groupBy(\"fighter_name\").agg(\n",
        "            F.avg(\"search_interest\").alias(\"avg_search_interest\"),\n",
        "            F.max(\"search_interest\").alias(\"max_search_interest\"),\n",
        "            F.stddev(\"search_interest\").alias(\"search_volatility\")\n",
        "        )\n",
        "        trends_agg = trends_agg.withColumn(\"fighter_name_norm\", F.lower(F.regexp_replace(F.col(\"fighter_name\"), r\"[^a-z0-9]\", \"\")))\n",
        "        print(f\"  Processed {trends_agg.count()} trend records\")\n",
        "        return trends_agg\n",
        "    return None\n",
        "\n",
        "print(\"Feature processing functions defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 9: Define enrichment and save functions\n",
        "\n",
        "def enrich_fights_with_external(fights_df, betting_df, trends_df, graph_df):\n",
        "    print(\"Enriching fights with external data...\")\n",
        "    df = fights_df\n",
        "    df = df.withColumn(\"f1_name_norm\", F.lower(F.regexp_replace(F.col(\"fighter1_name\"), r\"[^a-z0-9]\", \"\")))\n",
        "    df = df.withColumn(\"f2_name_norm\", F.lower(F.regexp_replace(F.col(\"fighter2_name\"), r\"[^a-z0-9]\", \"\")))\n",
        "    \n",
        "    if betting_df is not None:\n",
        "        df = df.join(betting_df, on=[\"f1_name_norm\", \"f2_name_norm\"], how=\"left\")\n",
        "        print(\"  Added betting features\")\n",
        "    \n",
        "    if trends_df is not None and \"f1_name_norm\" in trends_df.columns:\n",
        "        df = df.join(trends_df, on=[\"f1_name_norm\", \"f2_name_norm\"], how=\"left\")\n",
        "        print(\"  Added trends features\")\n",
        "    \n",
        "    if graph_df is not None:\n",
        "        graph_df = graph_df.withColumn(\"name_norm\", F.lower(F.regexp_replace(F.col(\"name\"), r\"[^a-z0-9]\", \"\")))\n",
        "        graph_f1 = graph_df.select(\n",
        "            F.col(\"name_norm\").alias(\"f1_name_norm\"),\n",
        "            F.col(\"pagerank_score\").alias(\"f1_pagerank\"),\n",
        "            F.col(\"num_opponents\").alias(\"f1_network_size\"),\n",
        "            F.col(\"community_id\").alias(\"f1_community\")\n",
        "        )\n",
        "        df = df.join(graph_f1, on=\"f1_name_norm\", how=\"left\")\n",
        "        graph_f2 = graph_df.select(\n",
        "            F.col(\"name_norm\").alias(\"f2_name_norm\"),\n",
        "            F.col(\"pagerank_score\").alias(\"f2_pagerank\"),\n",
        "            F.col(\"num_opponents\").alias(\"f2_network_size\"),\n",
        "            F.col(\"community_id\").alias(\"f2_community\")\n",
        "        )\n",
        "        df = df.join(graph_f2, on=\"f2_name_norm\", how=\"left\")\n",
        "        df = df.withColumn(\"combined_pagerank\", F.coalesce(F.col(\"f1_pagerank\"), F.lit(0.0)) + F.coalesce(F.col(\"f2_pagerank\"), F.lit(0.0)))\n",
        "        df = df.withColumn(\"combined_network_size\", F.coalesce(F.col(\"f1_network_size\"), F.lit(0)) + F.coalesce(F.col(\"f2_network_size\"), F.lit(0)))\n",
        "        df = df.withColumn(\"same_community\", (F.col(\"f1_community\") == F.col(\"f2_community\")).cast(IntegerType()))\n",
        "        print(\"  Added graph features\")\n",
        "    \n",
        "    df = df.fillna({\n",
        "        \"betting_spread\": 0.1, \"is_competitive\": True, \"has_heavy_favorite\": False,\n",
        "        \"pre_event_buzz_7d\": 50.0, \"reddit_sentiment\": 0.0, \"combined_pagerank\": 0.5, \"combined_network_size\": 10\n",
        "    })\n",
        "    print(f\"  Enriched {df.count()} fights\")\n",
        "    return df\n",
        "\n",
        "def enrich_events_with_sentiment(events_df, sentiment_df):\n",
        "    if sentiment_df is None:\n",
        "        return events_df\n",
        "    print(\"Enriching events with sentiment...\")\n",
        "    events_df = events_df.withColumn(\"event_name_norm\", F.lower(F.regexp_replace(F.col(\"event_name\"), r\"[^a-z0-9]\", \"\")))\n",
        "    df = events_df.join(sentiment_df, on=\"event_name_norm\", how=\"left\")\n",
        "    df = df.fillna({\"reddit_sentiment\": 0.0, \"reddit_engagement\": 100, \"reddit_hype\": 0.5})\n",
        "    print(f\"  Enriched {df.count()} events\")\n",
        "    return df\n",
        "\n",
        "def save_parquet(df, output_path, partition_cols=None):\n",
        "    print(f\"Saving to {output_path}...\")\n",
        "    writer = df.write.mode(\"overwrite\")\n",
        "    if partition_cols:\n",
        "        writer = writer.partitionBy(*partition_cols)\n",
        "    writer.parquet(output_path)\n",
        "    print(f\"  Saved {df.count()} rows\")\n",
        "\n",
        "print(\"Enrichment and save functions defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 10: Load all data\n",
        "raw_dfs = load_data(spark, DATA_DIR)\n",
        "print(f\"\\nLoaded datasets: {list(raw_dfs.keys())}\")\n",
        "\n",
        "external_dfs = load_external_data(spark, DATA_DIR)\n",
        "print(f\"\\nLoaded external datasets: {list(external_dfs.keys())}\")\n",
        "\n",
        "graph_features = load_graph_features(spark, DATA_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 11: Clean the data\n",
        "\n",
        "# Clean events\n",
        "events_clean = None\n",
        "if \"events\" in raw_dfs:\n",
        "    events_clean = clean_events(raw_dfs[\"events\"])\n",
        "    print(f\"Events cleaned: {events_clean.count()} rows\")\n",
        "    events_clean.show(5)\n",
        "else:\n",
        "    print(\"ERROR: No events data!\")\n",
        "\n",
        "# Clean fighters\n",
        "fighters_clean = None\n",
        "if \"fighter_details\" in raw_dfs:\n",
        "    fighters_clean = clean_fighters(raw_dfs[\"fighter_details\"])\n",
        "elif \"fighters\" in raw_dfs:\n",
        "    fighters_clean = clean_fighters(raw_dfs[\"fighters\"])\n",
        "\n",
        "if fighters_clean:\n",
        "    print(f\"Fighters cleaned: {fighters_clean.count()} rows\")\n",
        "    fighters_clean.show(5)\n",
        "else:\n",
        "    print(\"Warning: No fighter data found\")\n",
        "\n",
        "# Clean fights\n",
        "fights_clean = None\n",
        "if \"fight_details\" in raw_dfs:\n",
        "    fights_clean = clean_fights(raw_dfs[\"fight_details\"])\n",
        "elif \"fight_results\" in raw_dfs:\n",
        "    fights_clean = clean_fights(raw_dfs[\"fight_results\"])\n",
        "\n",
        "if fights_clean:\n",
        "    print(f\"Fights cleaned: {fights_clean.count()} rows\")\n",
        "    fights_clean.show(5)\n",
        "else:\n",
        "    print(\"Warning: No fight data found\")\n",
        "\n",
        "# Clean fight stats\n",
        "fight_stats_clean = None\n",
        "if \"fight_stats\" in raw_dfs:\n",
        "    fight_stats_clean = clean_fight_stats(raw_dfs[\"fight_stats\"])\n",
        "    print(f\"Fight stats cleaned: {fight_stats_clean.count()} rows\")\n",
        "else:\n",
        "    print(\"Warning: No fight stats data found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 12: Process external data\n",
        "\n",
        "betting_features = None\n",
        "if \"betting_odds\" in external_dfs:\n",
        "    betting_features = process_betting_odds(external_dfs[\"betting_odds\"], fights_clean)\n",
        "\n",
        "sentiment_features = None\n",
        "if \"event_sentiment\" in external_dfs:\n",
        "    sentiment_features = process_sentiment(external_dfs[\"event_sentiment\"], events_clean)\n",
        "\n",
        "trends_features = None\n",
        "if \"fighter_buzz\" in external_dfs:\n",
        "    trends_features = process_trends(\n",
        "        external_dfs.get(\"google_trends\"),\n",
        "        external_dfs[\"fighter_buzz\"]\n",
        "    )\n",
        "elif \"google_trends\" in external_dfs:\n",
        "    trends_features = process_trends(external_dfs[\"google_trends\"], None)\n",
        "\n",
        "print(\"\\nExternal data processing complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 13: Enrich data and build features\n",
        "\n",
        "# Enrich fights with external data\n",
        "fights_enriched = fights_clean\n",
        "if fights_clean is not None:\n",
        "    fights_enriched = enrich_fights_with_external(\n",
        "        fights_clean,\n",
        "        betting_features,\n",
        "        trends_features,\n",
        "        graph_features\n",
        "    )\n",
        "\n",
        "# Enrich events with sentiment\n",
        "events_enriched = events_clean\n",
        "if sentiment_features is not None:\n",
        "    events_enriched = enrich_events_with_sentiment(events_clean, sentiment_features)\n",
        "\n",
        "# Build event features\n",
        "if fights_enriched:\n",
        "    event_features = build_event_features(events_enriched, fights_enriched)\n",
        "else:\n",
        "    event_features = events_enriched\n",
        "\n",
        "print(\"\\nEnrichment complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 14: Save to Parquet\n",
        "\n",
        "processed_dir = os.path.join(OUTPUT_DIR, \"processed\")\n",
        "os.makedirs(processed_dir, exist_ok=True)\n",
        "\n",
        "save_parquet(\n",
        "    events_enriched,\n",
        "    os.path.join(processed_dir, \"events\"),\n",
        "    partition_cols=[\"year\"]\n",
        ")\n",
        "\n",
        "if fighters_clean:\n",
        "    save_parquet(\n",
        "        fighters_clean,\n",
        "        os.path.join(processed_dir, \"fighters\")\n",
        "    )\n",
        "\n",
        "if fights_enriched:\n",
        "    save_parquet(\n",
        "        fights_enriched,\n",
        "        os.path.join(processed_dir, \"fights\")\n",
        "    )\n",
        "\n",
        "if fight_stats_clean:\n",
        "    save_parquet(\n",
        "        fight_stats_clean,\n",
        "        os.path.join(processed_dir, \"fight_stats\")\n",
        "    )\n",
        "\n",
        "save_parquet(\n",
        "    event_features,\n",
        "    os.path.join(processed_dir, \"event_features\"),\n",
        "    partition_cols=[\"year\"]\n",
        ")\n",
        "\n",
        "print(f\"\\nAll data saved to: {processed_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 15: Summary and cleanup\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"ETL PIPELINE COMPLETE\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"  Betting data: {'Yes' if betting_features is not None else 'No'}\")\n",
        "print(f\"  Trends data: {'Yes' if trends_features is not None else 'No'}\")\n",
        "print(f\"  Sentiment data: {'Yes' if sentiment_features is not None else 'No'}\")\n",
        "print(f\"  Graph data: {'Yes' if graph_features is not None else 'No'}\")\n",
        "print(f\"\\nOutput saved to: {processed_dir}\")\n",
        "\n",
        "# Stop Spark session\n",
        "spark.stop()\n",
        "print(\"\\nSpark session stopped.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Download processed files to local machine\n",
        "# Uncomment the lines below if you want to download the results\n",
        "\n",
        "# import shutil\n",
        "# from google.colab import files\n",
        "# \n",
        "# # Zip the processed directory\n",
        "# shutil.make_archive('/content/processed_data', 'zip', processed_dir)\n",
        "# \n",
        "# # Download the zip file\n",
        "# files.download('/content/processed_data.zip')\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
